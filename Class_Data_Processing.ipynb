{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Class_Data_Processing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeGbHd8Rpo4n",
        "colab_type": "text"
      },
      "source": [
        "## Homework 1 -  PROCESS FLOW CLASSES "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GjMSalipo4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "3b86ac98-bab4-45f0-e6c6-a3e8b7d2be08"
      },
      "source": [
        "# Import Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from sklearn import preprocessing\n",
        "from scipy import stats\n",
        "import scipy.stats as ttest_ind\n",
        "import scipy.stats as shapiro\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn import tree\n",
        "import scipy.stats as ttest_ind\n",
        "import scipy.stats as shapiro\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,log_loss\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict\n",
        "from scipy.stats.stats import pearsonr\n",
        "from scipy.stats.stats import spearmanr\n",
        "from sklearn.metrics import roc_auc_score,roc_curve\n",
        "import warnings \n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/6c/6608210b29649267de52001b09e369777ee2a5cfe1c71fa75eba82a4f2dc/catboost-0.24-cp36-none-manylinux1_x86_64.whl (65.9MB)\n",
            "\u001b[K     |████████████████████████████████| 65.9MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3U4-hMfqFxk",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f1f24af5-a0d1-4930-f883-c5b4145029c6"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1657e7a9-d024-4553-ad46-e0a97a785d49\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1657e7a9-d024-4553-ad46-e0a97a785d49\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving hmelq.csv to hmelq.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPkPVByNhjzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading Data\n",
        "df=pd.read_csv(\"hmelq.csv\")\n",
        "data=df.copy()\n",
        "class Loading_Data:\n",
        "    def __init__ (self,data):\n",
        "        self.data=data\n",
        "    def translate_to_dataframe(self):# dataset is converted to dataframe\n",
        "        return pd.DataFrame(self.data).head()# the first 5 observations of the data set are shown\n",
        "# Data Information\n",
        "class Information:\n",
        "    def __init__ (self,data):\n",
        "        self.data=data\n",
        "    def info_data(self):\n",
        "        print(self.data.head())\n",
        "        print(self.data.info())    \n",
        "        print(self.data.dtypes)\n",
        "        print(self.data.shape)\n",
        "        print(self.data.columns)\n",
        "    def describe_missing_values(self):\n",
        "        print(self.data.isnull().values.any()) # Are there any missing observations in the dataset? If there is True; If there is no False returns\n",
        "        print(self.data.isnull().sum())# Prints the number of missing observations on the basis of variables\n",
        "    def select_dtypes_numeric(self):\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])# numeric variables are selected\n",
        "        print(df_numeric)\n",
        "    def describe_data(self):# summary statistics information of numeric variables are accessed\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])# numeric variables are selected\n",
        "        print(self.data.describe().T)\n",
        "    def select_dtypes_category(self):\n",
        "        df_category=self.data.select_dtypes(include=[\"object\"])# categorical variables are selected\n",
        "        print(df_category)\n",
        "        for i in df_category.columns:\n",
        "            print(self.data[i].value_counts()) # Prints the frequency information of categorical variables\n",
        "    def data_cor(self):\n",
        "        print(self.data.corr())\n",
        "    def print_shape(self):\n",
        "      print(\"X_train shape\",X_train.shape)\n",
        "      print(\"y_train shape\",y_train.shape)\n",
        "      print(\"X_test shape\",X_test.shape)\n",
        "      print(\"y_test shape \",y_test.shape)\n",
        "\n",
        "# Exploratory Data Analysis (EDA)\n",
        "class Visualizer:\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "    def msno_bar(self):\n",
        "        plt.figure(figsize=(6,5))\n",
        "        msno_bar = msno.bar(self.data,color='lightblue')\n",
        "        return msno_bar\n",
        "    def heat_map(self):\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        f,ax = plt.subplots(figsize=(200,200))\n",
        "        return sns.heatmap(df_numeric.corr(), annot=True, linewidths=.8, fmt='.8f', ax=ax)\n",
        "    def bar_plot(self,x=None,y=None,z = None):# Used to visualize barplot categorical variables\n",
        "        plt.figure(figsize=(6,5))\n",
        "        sns.barplot(x=x, y=y, hue=z, data=self.data)\n",
        "    def box_plot(self,x=None,y=None,z=None): # continuous variables are visualized with the help of cartridges\n",
        "        numeric_features=[x for x in data.columns if data[x].dtype!=\"object\"]\n",
        "        for i in data[numeric_features].columns:\n",
        "            plt.figure(figsize=(6,5))\n",
        "            plt.title(i)\n",
        "            sns.boxplot(data=data[i])\n",
        "    def hist_plot(self):\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        for i in df_numeric.columns:\n",
        "            plt.figure()\n",
        "            plt.hist(df_numeric[i],bins=100,color=\"orange\")\n",
        "            plt.title(\"Histogram of \"+ i)\n",
        "    def plot_tree(self,model_name=None):\n",
        "      fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (5,6), dpi=500)\n",
        "      tree.plot_tree(model_name,filled = True);\n",
        "      text_representation = tree.export_text(model_name)\n",
        "      print(text_representation)\n",
        "    def dist_plot(self,x=None,y=None,z=None):\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        df_numeric=df_numeric.dropna()\n",
        "        for i in  df_numeric.columns:\n",
        "            plt.figure()\n",
        "            sns.distplot(np.array(df_numeric[i]),hist=False,kde=True,color=\"g\")\n",
        "            plt.title(\"Distplot  of \"+ i)\n",
        "    def reg_plot(self):\n",
        "        plt.figure(figsize=(16, 7))\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        for i, column in enumerate(df_numeric.select_dtypes(exclude=['object']).columns[1:], 1):\n",
        "            plt.subplot(2, 5, i)\n",
        "            randNorm = np.random.normal(np.mean(df_numeric[column]), np.std(df_numeric[column]), len(df_numeric[column]))\n",
        "            sns.regplot(np.sort(randNorm), np.sort(df_numeric[column]))\n",
        "            plt.xlabel(f'{column}')\n",
        "    def count_plot(self,x=None,y=None,z=None):\n",
        "        plt.figure(figsize=(6,5))\n",
        "        sns.countplot(x=x, y=y, hue=z, data=self.data)\n",
        "    def correlation(self):\n",
        "        fig,ax = plt.subplots(figsize=(10, 10))\n",
        "        sns.heatmap(data.corr(), ax=ax, annot=True, \n",
        "        linewidths=0.05, fmt= '.2f',cmap=\"Blues\")\n",
        "        plt.show()\n",
        "    def scatter_plot(self,x=None,y=None,z=None):\n",
        "        return sns.scatterplot(x=x,y=y,data=self.data)\n",
        "    def lm_plot(self,x=None,y=None,z=None,w=None,r=None):\n",
        "        return sns.lmplot(x=x, y=y, hue=z,col=w,row=r, data=self.data)\n",
        "    def swarm_plot(self,x=None,y=None,z=None):\n",
        "        return sns.swarmplot(x=x, y=y,hue=z, data=self.data)\n",
        "    def line_plot(self,x=None,y=None,z=None):\n",
        "        return sns.lineplot(x=x,y=y,hue=z,data=self.data)\n",
        "    def pair_plot(self,x=None,y=None,z=None,w=None):\n",
        "        return sns.pairplot(self.data,hue=z)\n",
        "    def cross_tab(self,x=None,y=None,n=None):\n",
        "        numeric_features=[x for x in data.columns if data[x].dtype!=\"object\"]\n",
        "        for i in numeric_features.columns:\n",
        "            return pd.crosstab(self.data[i],self.data[i],normalize=n).style.background_gradient(cmap=\"summer_r\")\n",
        "    def plot_roc_curve(self,model_adı=None,X_test=None):\n",
        "      auc=roc_auc_score(y_test,model_adı.predict(X_test))\n",
        "      fpr,tpr,threshold=roc_curve(y_test,model_adı.predict_proba(X_test)[:,1])\n",
        "      plt.figure()\n",
        "      plt.plot(fpr,tpr,label='AUC( area =%0.2f)' % auc)\n",
        "      plt.plot([0,1],[0,1],'r--')\n",
        "      plt.xlim([0.0,1.0])\n",
        "      plt.ylim([0.0,1.05])\n",
        "      plt.xlabel('False Positive Oranı')\n",
        "      plt.ylabel('True Positive Oranı')\n",
        "      plt.title('ROC')\n",
        "      plt.show()  \n",
        "    def degisken_onem_görsellestirme(self,model_name=None,X_train=None):\n",
        "      Importance=pd.DataFrame({\"Importance\":model_name.feature_importances_*100},index=X_train.columns)\n",
        "      Importance.sort_values(by=\"Importance\",axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\n",
        "      plt.xlabel(\"Degiskenlerin Önem Düzeyleri\")\n",
        "# Performing Hypothesis Testing\n",
        "class HypothesisTesting:\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "    def normality_assumption(self):# normality assumption is realized by shapiro wilks test\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        for i in df_numeric.columns:\n",
        "                df_new = df_numeric.dropna(subset=[i])\n",
        "                stat, p = stats.shapiro(df_new[i])\n",
        "                print(\"Statistics:%3.3f, p=%.3f \" % (stat,p))\n",
        "                alpha = 0.05\n",
        "                if p>alpha:\n",
        "                    print(i,\" için Orneklem Normal (Gaussian) Dagilimdan gelmektedir (Fail to Reject H0)\")\n",
        "                else:\n",
        "                    print(i,\" için Orneklem Normal (Gaussian) Dagilimdan gelmemektedir (reject H0)\")\n",
        "        print(\"*****************************************************************************************\")\n",
        "    def assumption_of_variance_homogeneity(self,variable=None,x=None,y=None):#assumption of variance homogeneity is realized by levene test\n",
        "        grps=pd.unique(data[variable].values)\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        for i in df_numeric.columns:\n",
        "            for j in grps:\n",
        "                df_new = data.dropna(subset=[i])\n",
        "                stat, p = stats.levene(df_new[i][data[variable]==x],df_new[i][data[variable]==y])\n",
        "                print(\"Statistics:%3.3f, p=%.3f \" % (stat,p))\n",
        "                alpha = 0.05\n",
        "                if p>alpha:\n",
        "                    print(i,j,\" için varyans homojendir. (Fail to Reject H0)\")\n",
        "                else:\n",
        "                    print(i,j,\" için varyans homojen degildir. (reject H0)\")\n",
        "        print(\"*****************************************************************************************\")\n",
        "    # numeri amaç değişkeni - kategorik bagımsız değişken 2 sınıflı : paired t test\n",
        "    # numeri amaç değişkeni - kategorik bagımsız değişken 2 den fazla sınıflı : anova\n",
        "    def two_independent_samples_t_test(self,variable=None,x=None,y=None):\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        for i in df_numeric.columns:\n",
        "            df_new= self.data.dropna(subset=[i])\n",
        "            A=df_new[df_new[variable]==x][i]\n",
        "            B=df_new[df_new[variable]==y][i]\n",
        "            t, p = stats.ttest_ind(A, B, equal_var=False)\n",
        "            print(\"ttest_ind: i=%s t = %g  p = %g\" % (i,t, p))\n",
        "            alpha = 0.05\n",
        "            if p>alpha:\n",
        "                print(i,\"ile\", variable,\" değişkeni arasında istatistiksel olarak anlamlı bir fark vardır.(Fail to Reject H0)\")\n",
        "            else:\n",
        "                print(i,\" ile \", variable,\" değişkeni arasında istatistiksel olarak anlamlı bir fark yoktur.(reject H0)\")\n",
        "        print(\"*****************************************************************************************\")\n",
        "    # kategorik amaç değişkeni- kategorik bağımsız değişken : ki- kare bağımsızlık testi(chi-square)\n",
        "    def chi_square_t_test(self,x=None,y=None):\n",
        "        data_cross_tab=pd.crosstab(index=data[x],columns=data[y])\n",
        "        chi2,p,dof,expected=stats.chi2_contingency(data_cross_tab)\n",
        "        results=[[\"Item\",\"Value\"],\n",
        "                 [\"Chi-Square Test\",chi2],\n",
        "                 [\"p - value\",p]]\n",
        "        print(\"Chi-Square Test =%g p=%g\" %(chi2,p))\n",
        "        alpha = 0.05\n",
        "        if p>alpha:\n",
        "            print(x,\"ve\",y,\" degiskenleri birbirinden bağımsızdır.(Fail to Reject H0)\")\n",
        "        else:\n",
        "            print(x,\"ve\",y,\" degiskenleri birbirinden bağımsız değildir(reject H0)\")\n",
        "        print(\"*****************************************************************************************\")\n",
        "    def spearmanr_test(self,x=None):#  Korelasyon Anlamlılığı Testi için Spearman Testini kullanıcaz.---nonparametrik bir test\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        for i in df_numeric.columns:\n",
        "            test_istatistigi,pvalue=stats.spearmanr(self.data[x],self.data[i])\n",
        "            print('Korelasyon Katsayısı= %.4f, p- degeri=%.4f' % (test_istatistigi,pvalue))\n",
        "            alpha=0.05\n",
        "            if pvalue >alpha:\n",
        "                print('Degiskenler arasında anlamlı bir fark yoktur, (Fail to reject)')\n",
        "            else:\n",
        "                print('Degiskenler arasında anlamlı bir fark vardır, (Reject)')\n",
        "        print(\"*****************************************************************************************\")\n",
        "\n",
        "    #numeric amaç değişkeni- numeric bağımsız değişken : korelasyon analizi ve ilgili anlamlılık testi\n",
        "    def pearsonr_test(self,x=None):#Korelasyon analizini pearsonr ile de gerçekleştirdim--parametrik test\n",
        "        df_numeric=self.data.select_dtypes(include=['float64','int64'])\n",
        "        for i in df_numeric.columns:\n",
        "            test_istatistigi,pvalue=pearsonr(self.data[x],self.data[i])\n",
        "            print('Korelasyon Katsayısı= %.4f, p- degeri=%.4f' % (test_istatistigi,pvalue))\n",
        "            alpha=0.05\n",
        "            if pvalue >alpha:\n",
        "                print('Degiskenler arasında anlamlı bir fark yoktur, (Fail to reject)')\n",
        "            else:\n",
        "                print('Degiiskenler arasında anlamlı bir fark vardır, (Reject)')\n",
        "        print(\"*****************************************************************************************\")\n",
        "# Data Preprocessing\n",
        "class PreprocessStrategy:\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "    def fill_missing_value(self,data=None):\n",
        "        data=self.data.dropna()\n",
        "        return data.head()\n",
        "    def drop_feature(self,x=None,y=None,z=None,data=None):\n",
        "        X_ = self.data.drop([x, y, z],axis=1)\n",
        "        return X_\n",
        "    def select_feature(self,x=None):\n",
        "      y = self.data[x]\n",
        "      return y\n",
        "    def data_concat(self,x=None,y=None,z=None,a=None,b=None,c=None):\n",
        "      X = pd.concat([X_, dms[[x,y,z,a,b,c]]], axis = 1)\n",
        "      return X\n",
        "    def dataset_split(self,X=None,y=None,test_size=None,random_state=None):\n",
        "      X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=test_size,random_state=random_state)\n",
        "      print(\"X_train shape :\",X_train.shape)\n",
        "      print(\"X_test shape :\", X_test.shape)\n",
        "      print(\"y_train shape: \",y_train.shape)\n",
        "      print(\"y_test shape :\" ,y_test.shape)\n",
        "      return X_train,X_test,y_train,y_test\n",
        "    def replace_nan(self,x=None):\n",
        "        return self.data.replace(x,np.nan,inplace=True)\n",
        "    def fill_missing_specific_variable_with_median(self,X=None):#filling missing observations specific to the variable\n",
        "        return self.data[X].fillna(self.data[X].median(),inplace=True)\n",
        "    def fill_missing_value_with_mean(self):#fill in missing values in all variables with mean\n",
        "        return self.data.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
        "    def fill_missing_value_with_median(self):#fill in missing values in all variables with median\n",
        "        return self.data.apply(lambda x: x.fillna(x.median()),axis=0)\n",
        "    def normalization(self):# converts variable values from 0 to 1\n",
        "        return preprocessing.normalize(self.data)\n",
        "    def one_hot_dummy_variable(self,x=None,y=None):#It can be used to convert categorical variable to continuous variable. As a result, awareness among classes will be preserved.\n",
        "        df_one_hot=self.data.copy()\n",
        "        dms = pd.get_dummies(df_one_hot[[x, y]])\n",
        "        return dms\n",
        "    def label_encoder(self,new_variable_name=None,categorical_variable_to_converted=None):# Performs conversions by the number of classes available\n",
        "        lbe=preprocessing.LabelEncoder()\n",
        "        data[new_variable_name]=lbe.fit_transform(data[categorical_variable_to_converted])\n",
        "        return data[new_variable_name]\n",
        "    def standardization(self):#a standardization is performed with an average of 0 standard deviations of one\n",
        "        df_standardization=preprocessing.scale(self.data)\n",
        "        return df_standardization\n",
        "    def min_max_transformation(self,x=None,y=None):#Used to convert the values of a variable between two ranges that we want\n",
        "        scaler=preprocessing.MinMaxScaler(feature_range=(x,y))\n",
        "        return scaler.fit_transform(self.data)\n",
        "    def discarding_pvalue(self,X_train=None,X_test=None):\n",
        "      X_train = X_train[loj_model.pvalues[loj_model.pvalues <  0.05].index]\n",
        "      X_test=X_test[loj_model.pvalues[loj_model.pvalues <  0.05].index]\n",
        "      return X_train,X_test\n",
        "    def binarize_transformation(self,threshold=None):#Converts the variable's values to 0 or 1 according to a certain threshold value\n",
        "        binarizer=preprocessing.Binarizer(threshold=threshold).fit(self.data)\n",
        "        return binarizer.transform(self.data)\n",
        "  \n",
        "        \n",
        "# Data Modelling ,Performance/Evaluation metrics of the models\n",
        "class GridSearchHelper():# model fit, model predict and model results are performed in this section\n",
        "    def __init__(self,data):\n",
        "        self.data=data\n",
        "    def linear_regresyon(self,X_train=None,X_test=None,y_test=None,y_train=None):\n",
        "        #X= df[[x]]# x independent variable\n",
        "        #y=df[[y]] # y dependent variable \n",
        "        lm=LinearRegression()\n",
        "        linear_model=lm.fit(X_train,y_train)# model object created\n",
        "        return linear_model\n",
        "    def pca(self,X_train=None,X_test=None,y_test=None,y_train=None):\n",
        "        pca =PCA()\n",
        "        X_reduced_test= pca.fit_transform(scale(X_test))\n",
        "        print(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)[0:5])\n",
        "        lm=LinearRegression()\n",
        "        y_train=y_train.fillna(y_train.mean())\n",
        "        pca_model=lm.fit(X_reduced_test,y_train)\n",
        "        return pca_model\n",
        "    def logistic_regresyon(self,X_train=None,y_train=None,solver=None):\n",
        "        loj=LogisticRegression(solver=solver)\n",
        "        loj_model=loj.fit(X_train,y_train)\n",
        "        return loj_model\n",
        "    def loj_summary(self,X_train=None,y_train=None):\n",
        "      loj = sm.Logit(y_train, X_train)\n",
        "      loj_model = loj.fit()\n",
        "      print(loj_model.summary())\n",
        "    def decision_tree(self,X_train=None,y_train=None):\n",
        "      cart=DecisionTreeClassifier()\n",
        "      cart_model=cart.fit(X_train,y_train)\n",
        "      print(cart_model)\n",
        "      return cart_model\n",
        "    def random_forest(self,X_train=None,y_train=None):\n",
        "      rf=RandomForestClassifier()\n",
        "      rf_model=rf.fit(X_train,y_train)\n",
        "      print(rf_model)\n",
        "      return rf_model\n",
        "    def decision_tree_model_tuning(self,X_train=None,y_train=None):\n",
        "      decision_tree_classifier_grid={\"max_depth\": range(1,10),\"criterion\":[\"gini\",\"entropy\"],\"min_samples_split\":list(range(2,15))}\n",
        "      cart=tree.DecisionTreeClassifier()\n",
        "      cart_cv=GridSearchCV(cart,decision_tree_classifier_grid,cv=10,n_jobs=-1,verbose=2)\n",
        "      cart_cv_model=cart_cv.fit(X_train,y_train)\n",
        "      cart_classifier=tree.DecisionTreeClassifier(max_depth=cart_cv_model.best_params_[\"max_depth\"],min_samples_split=cart_cv_model.best_params_[\"min_samples_split\"],criterion=cart_cv_model.best_params_[\"criterion\"])\n",
        "      cart_tuned=cart_classifier.fit(X_train,y_train)\n",
        "      print(cart_tuned)\n",
        "      return cart_tuned\n",
        "    def random_forest_model_tuning(self,X_train=None,y_train=None):\n",
        "      rf_params={\"max_depth\":[3,5,8],\"max_features\":[0.1,0.25,0.5],\"n_estimators\":[200,500,1000],\"min_samples_split\":[5,10]}# göz öünde bulunacak olan degsken sayısı\n",
        "      rf_cv_model=GridSearchCV(rf_model,rf_params,cv=10,n_jobs=-1,verbose=2)\n",
        "      rf_cv_model.fit(X_train,y_train)\n",
        "      rf_tuned=RandomForestClassifier(max_depth=rf_cv_model.best_params_[\"max_depth\"],max_features=rf_cv_model.best_params_[\"max_features\"],min_samples_split= rf_cv_model.best_params_[\"min_samples_split\"],n_estimators= rf_cv_model.best_params_[\"n_estimators\"])\n",
        "      rf_tuned=rf_tuned.fit(X_train,y_train)\n",
        "      print(rf_tuned)\n",
        "      return rf_tuned\n",
        "    def neural_network(self,X_train=None,y_train=None):\n",
        "      mlpc=MLPClassifier()\n",
        "      mlpc_model=mlpc.fit(X_train_scaled,y_train)\n",
        "      print(mlpc_model)\n",
        "      return mlpc_model\n",
        "    def neural_network_model_tuning(self,X_train=None,y_train=None):\n",
        "      mlpc_params={\"alpha\":[0.1,0.01,0.02],\n",
        "                   \"hidden_layer_sizes\":[(100,100,100),(100,100),(3,5),(5,3)],\"solver\":[\"adam\",\"sgd\"],\"activation\":[\"relu\",\"logistic\"]}#activation function\n",
        "      mlpc=MLPClassifier()\n",
        "      mlpc_cv_model=GridSearchCV(mlpc,mlpc_params,cv=10,n_jobs=-1,verbose=2)\n",
        "      mlpc_cv_model.fit(X_train_scaled,y_train)\n",
        "      mlpc_tuned=MLPClassifier(alpha=mlpc_cv_model.best_params_[\"alpha\"],\n",
        "                        hidden_layer_sizes=mlpc_cv_model.best_params_[\"hidden_layer_sizes\"],\n",
        "                        solver=mlpc_cv_model.best_params_[\"solver\"],\n",
        "                        activation=mlpc_cv_model.best_params_[\"activation\"])\n",
        "      mlpc_tuned=mlpc_tuned.fit(X_train_scaled,y_train)\n",
        "      print(mlpc_tuned)\n",
        "      return mlpc_tuned\n",
        "\n",
        "    def support_vektor_machine(self,X_train=None,y_train=None):\n",
        "      svm=SVC()\n",
        "      svm_model=svm.fit(X_train,y_train)\n",
        "      print(svm_model)\n",
        "      return svm_model\n",
        "    def svc_model_tuning(self,X_train=None,y_train=None):\n",
        "      svc_params={\"C\":np.arange(2,5),\"kernel\":[\"linear\",\"rbf\"]}\n",
        "      svc=SVC()\n",
        "      svc_cv_model=GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose=2)\n",
        "      svc_cv_model.fit(X_train,y_train)\n",
        "      svc_tuned=SVC(kernel=svc_cv_model.best_params_[\"kernel\"],C=svc_cv_model.best_params_[\"C\"]).fit(X_train,y_train)\n",
        "      print(svc_tuned)\n",
        "      return svc_tuned\n",
        "    def gaussian_naive_bayes(self,X_train=None,y_train=None):\n",
        "      naive_bayes=GaussianNB()\n",
        "      nb_model=naive_bayes.fit(X_train,y_train)\n",
        "      print(nb_model)\n",
        "      return nb_model\n",
        "    def multinominal_naive_bayes(self,X_train=None,y_train=None):\n",
        "      mnb = MultinomialNB()\n",
        "      mnb_model = mnb.fit(X_train, y_train)\n",
        "      print(mnb_model)\n",
        "      return mnb_model\n",
        "    def XGBoost(self,X_train=None,y_train=None):\n",
        "      xgb_model=XGBClassifier()\n",
        "      xgb_model.fit(X_train,y_train)\n",
        "      print(xgb_model)\n",
        "      return xgb_model\n",
        "    def xgb_model_tuning(self,X_train,y_train):\n",
        "      xgb_params={\"n_estimators\":[100,500,1000,2000],\"subsample\":[0.6,0.8,1.0],\"max_depth\":[3,4,5,6],\"learning_rate\":[0.1,0.01,0.02,0.05],\"min_samples_split\":[2,5,10]}\n",
        "      xgb=XGBClassifier()\n",
        "      xgb_cv_model=GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,verbose=2)\n",
        "      xgb_cv_model.fit(X_train,y_train)\n",
        "      xgb=XGBClassifier(n_estimators=xgb_cv_model.best_params_[\"n_estimators\"],subsample=xgb_cv_model.best_params_[\"subsample\"],max_depth=xgb_cv_model.best_params_[\"max_depth\"],learning_rate=xgb_cv_model.best_params_[\"learning_rate\"],min_samples_split=xgb_cv_model.best_params_[\"min_samples_split\"])\n",
        "      xgb_tuned=xgb.fit(X_train,y_train)\n",
        "      print(xgb_tuned)\n",
        "      return xgb_tuned\n",
        "    def knn(self,X_train=None,y_train=None):\n",
        "      knn=KNeighborsClassifier()\n",
        "      knn_model=knn.fit(X_train,y_train)\n",
        "      print(knn_model)\n",
        "      return knn_model\n",
        "    def knn_model_tuning(self,X_train=None,y_train=None):\n",
        "      knn_params={\"n_neighbors\":np.arange(2,40),\"metric\":[\"minkowski\",\"euclidean\",\"manhattan\"]}\n",
        "      knn=KNeighborsClassifier()\n",
        "      knn_cv= GridSearchCV(knn,knn_params,cv=10)\n",
        "      knn_cv.fit(X_train,y_train)\n",
        "      knn_tuned=KNeighborsClassifier(n_neighbors=knn_cv.best_params_[\"n_neighbors\"],metric=knn_cv.best_params_[\"metric\"])\n",
        "      knn_tuned.fit(X_train,y_train)\n",
        "      print(knn_tuned)\n",
        "      return knn_tuned\n",
        "    def coefficiant(self,model_name=None):\n",
        "      print(\"Sabit katsayı: \",loj_model.intercept_)# sabit katsayı\n",
        "      print(\"katsayılar :\",loj_model.coef_)# katsayılar\n",
        "    def fit_predict_and_show_result(self,model_name=None,X_test=None):\n",
        "      y_pred=model_name.predict(X_test)\n",
        "      print(\"Classification Report\",classification_report(y_test, y_pred))\n",
        "      print(\"Accuracy Score \",accuracy_score(y_test, y_pred))\n",
        "      print(\"roc_auc\",roc_auc_score(y_test,model_name.predict(X_test)))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlxAdA2Cpo4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}