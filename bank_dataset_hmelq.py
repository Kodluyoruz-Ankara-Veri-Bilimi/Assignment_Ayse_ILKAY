# -*- coding: utf-8 -*-
"""bank_dataset_hmelq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jm9282fpL5h7st026GVtzpV2jKvsmJOx
"""

# Veri seti Acıklaması
# Veri seti Adı: Hmelq bir banka verisidir. İnsanların çeşitli niteliklere göre kredilerini ödeyip ödemedigi 
# bilgilerini tutar

#*BAD:The response(or Y) variable is BAD, which is coded as 0 (good risk) or 1 (bad risk).

#*LOAN: The amount of the loan requested( İstenen kredi miktarı) 
#(Daha az kredi talep eden adayların gelecekteki kredilerde temerrüde düşmeme ihtimalleri daha yüksek olmalıdır)

#*MORTDUE: How much the customer needs to pay on their mortgage(Müşterinin ipotek ödemesinde ne kadar ödeme yapması gerekiyor)
#(Daha az ipoteğe sahip olan başvuru sahiplerinin gelecekteki kredilerde temerrüde düşmeme ihtimalleri daha yüksek) 

#*VALUE: Assessed valuation (Değerlendirilen değer, geçerli vergileri ölçmek için bir mülke atanan dolar değeridir)
#(Mevcut mülkün değeri daha düşükse, gelecekteki kredilerde temerrüde düşme olasılığı daha az olmalıdır)

#*REASON: Debt consolidation or home improvement (DebtCon or HomeImp) (Ödemesi gelen borcu ertelemekj için alınan kredi)
#(Krediyi kullanma sebebinin temerrüde düşme şansı üzerinde de etkisi olabilir)

#*JOB: Broad job category
#*YOJ: Years on the job

#*DEROG: Number of derogatory reports (Negatif bir rapor , ödemeni geç yaparsan , ödemezsen vb.)
#(Aşağılayıcı raporlar kredi geçmişinde her zaman olumsuz bir işarettir ve bu sütundaki herhangi 
 #bir sayı gelecekte yüksek varsayılan temerrüt şansını göstermelidir.)
    
#*DELINQ: The number of delinquent trade lines (or credit accounts) (Aylık ödemesini 30günden fazla geciktirme sayısı)
#(Bir kişi geçmişte birkaç kez temerrüde düşmüş olsaydı, temerrüt şansı daha yüksek olurdu)
#(Ödenmemiş borçların sayısı)

#*CLAGE: Age of oldest trade line (oldest tradeline= credit account)
#*NINQ: Number of recent credit inquiries (Soft inquiries are usually made when you request a credit report or credit score for yourself.)
#*CLNO: Number of trade lines

#*DEBTINC: Debt to income ratio (Borç - Gelir oranı. Aylık toplam borcunun aylık toplam gelirine bölümü ile çıkarılan yüzdelik oran)
#(Bir kişinin gelir konusunda yüksek borcu varsa, o kişinin daha fazla borç geri ödemesi zor olacaktır, 
#bu nedenle bu rakamdaki yüksek bir rakam temerrüte düşmesi daha fazladır)

"""# Dataset"""

from google.colab import files
uploaded = files.upload()

# Import Packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno
from sklearn import preprocessing
from scipy import stats
from sklearn import tree
import scipy.stats as ttest_ind
import scipy.stats as shapiro
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import statsmodels.api as sm
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.preprocessing import scale
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,log_loss
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
!pip install catboost
from catboost import CatBoostClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict
from scipy.stats.stats import pearsonr
from scipy.stats.stats import spearmanr
from sklearn.metrics import roc_auc_score,roc_curve
import warnings 
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
warnings.filterwarnings("ignore")

# Loading Data
df=pd.read_csv("hmelq.csv")
data=df.copy()
class Loading_Data:
    def __init__ (self,data):
        self.data=data
    def translate_to_dataframe(self):# dataset is converted to dataframe
        return pd.DataFrame(self.data).head()# the first 5 observations of the data set are shown
# Data Information
class Information:
    def __init__ (self,data):
        self.data=data
    def info_data(self):
        print(self.data.head())
        print(self.data.info())    
        print(self.data.dtypes)
        print(self.data.shape)
        print(self.data.columns)
    def describe_missing_values(self):
        print(self.data.isnull().values.any()) # Are there any missing observations in the dataset? If there is True; If there is no False returns
        print(self.data.isnull().sum())# Prints the number of missing observations on the basis of variables
    def select_dtypes_numeric(self):
        df_numeric=self.data.select_dtypes(include=['float64','int64'])# numeric variables are selected
        print(df_numeric)
    def describe_data(self):# summary statistics information of numeric variables are accessed
        df_numeric=self.data.select_dtypes(include=['float64','int64'])# numeric variables are selected
        print(self.data.describe().T)
    def select_dtypes_category(self):
        df_category=self.data.select_dtypes(include=["object"])# categorical variables are selected
        print(df_category)
        for i in df_category.columns:
            print(self.data[i].value_counts()) # Prints the frequency information of categorical variables
    def data_cor(self):
        print(self.data.corr())
    def print_shape(self):
      print("X_train shape",X_train.shape)
      print("y_train shape",y_train.shape)
      print("X_test shape",X_test.shape)
      print("y_test shape ",y_test.shape)

# Exploratory Data Analysis (EDA)
class Visualizer:
    def __init__(self,data):
        self.data=data
    def msno_bar(self):
        plt.figure(figsize=(6,5))
        msno_bar = msno.bar(self.data,color='lightblue')
        return msno_bar
    def heat_map(self):
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        f,ax = plt.subplots(figsize=(200,200))
        return sns.heatmap(df_numeric.corr(), annot=True, linewidths=.8, fmt='.8f', ax=ax)
    def bar_plot(self,x=None,y=None,z = None):# Used to visualize barplot categorical variables
        plt.figure(figsize=(6,5))
        sns.barplot(x=x, y=y, hue=z, data=self.data)
    def box_plot(self,x=None,y=None,z=None): # continuous variables are visualized with the help of cartridges
        numeric_features=[x for x in data.columns if data[x].dtype!="object"]
        for i in data[numeric_features].columns:
            plt.figure(figsize=(6,5))
            plt.title(i)
            sns.boxplot(data=data[i])
    def hist_plot(self):
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        for i in df_numeric.columns:
            plt.figure()
            plt.hist(df_numeric[i],bins=100,color="orange")
            plt.title("Histogram of "+ i)
    def plot_tree(self,model_name=None):
      fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (5,6), dpi=500)
      tree.plot_tree(model_name,filled = True);
      text_representation = tree.export_text(model_name)
      print(text_representation)
    def dist_plot(self,x=None,y=None,z=None):
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        df_numeric=df_numeric.dropna()
        for i in  df_numeric.columns:
            plt.figure()
            sns.distplot(np.array(df_numeric[i]),hist=False,kde=True,color="g")
            plt.title("Distplot  of "+ i)
    def reg_plot(self):
        plt.figure(figsize=(16, 7))
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        for i, column in enumerate(df_numeric.select_dtypes(exclude=['object']).columns[1:], 1):
            plt.subplot(2, 5, i)
            randNorm = np.random.normal(np.mean(df_numeric[column]), np.std(df_numeric[column]), len(df_numeric[column]))
            sns.regplot(np.sort(randNorm), np.sort(df_numeric[column]))
            plt.xlabel(f'{column}')
    def count_plot(self,x=None,y=None,z=None):
        plt.figure(figsize=(6,5))
        sns.countplot(x=x, y=y, hue=z, data=self.data)
    def correlation(self):
        fig,ax = plt.subplots(figsize=(10, 10))
        sns.heatmap(data.corr(), ax=ax, annot=True, 
        linewidths=0.05, fmt= '.2f',cmap="Blues")
        plt.show()
    def scatter_plot(self,x=None,y=None,z=None):
        return sns.scatterplot(x=x,y=y,data=self.data)
    def lm_plot(self,x=None,y=None,z=None,w=None,r=None):
        return sns.lmplot(x=x, y=y, hue=z,col=w,row=r, data=self.data)
    def swarm_plot(self,x=None,y=None,z=None):
        return sns.swarmplot(x=x, y=y,hue=z, data=self.data)
    def line_plot(self,x=None,y=None,z=None):
        return sns.lineplot(x=x,y=y,hue=z,data=self.data)
    def pair_plot(self,x=None,y=None,z=None,w=None):
        return sns.pairplot(self.data,hue=z)
    def cross_tab(self,x=None,y=None,n=None):
        numeric_features=[x for x in data.columns if data[x].dtype!="object"]
        for i in numeric_features.columns:
            return pd.crosstab(self.data[i],self.data[i],normalize=n).style.background_gradient(cmap="summer_r")
    def plot_roc_curve(self,model_adı=None,X_test=None):
      auc=roc_auc_score(y_test,model_adı.predict(X_test))
      fpr,tpr,threshold=roc_curve(y_test,model_adı.predict_proba(X_test)[:,1])
      plt.figure()
      plt.plot(fpr,tpr,label='AUC( area =%0.2f)' % auc)
      plt.plot([0,1],[0,1],'r--')
      plt.xlim([0.0,1.0])
      plt.ylim([0.0,1.05])
      plt.xlabel('False Positive Oranı')
      plt.ylabel('True Positive Oranı')
      plt.title('ROC')
      plt.show()  
    def degisken_onem_görsellestirme(self,model_name=None,X_train=None):
      Importance=pd.DataFrame({"Importance":model_name.feature_importances_*100},index=X_train.columns)
      Importance.sort_values(by="Importance",axis=0,ascending=True).plot(kind="barh",color="r")
      plt.xlabel("Degiskenlerin Önem Düzeyleri")
# Performing Hypothesis Testing
class HypothesisTesting:
    def __init__(self,data):
        self.data=data
    def normality_assumption(self):# normality assumption is realized by shapiro wilks test
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        for i in df_numeric.columns:
                df_new = df_numeric.dropna(subset=[i])
                stat, p = stats.shapiro(df_new[i])
                print("Statistics:%3.3f, p=%.3f " % (stat,p))
                alpha = 0.05
                if p>alpha:
                    print(i," için Orneklem Normal (Gaussian) Dagilimdan gelmektedir (Fail to Reject H0)")
                else:
                    print(i," için Orneklem Normal (Gaussian) Dagilimdan gelmemektedir (reject H0)")
        print("*****************************************************************************************")
    def assumption_of_variance_homogeneity(self,variable=None,x=None,y=None):#assumption of variance homogeneity is realized by levene test
        grps=pd.unique(data[variable].values)
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        for i in df_numeric.columns:
            for j in grps:
                df_new = data.dropna(subset=[i])
                stat, p = stats.levene(df_new[i][data[variable]==x],df_new[i][data[variable]==y])
                print("Statistics:%3.3f, p=%.3f " % (stat,p))
                alpha = 0.05
                if p>alpha:
                    print(i,j," için varyans homojendir. (Fail to Reject H0)")
                else:
                    print(i,j," için varyans homojen degildir. (reject H0)")
        print("*****************************************************************************************")
    # numeri amaç değişkeni - kategorik bagımsız değişken 2 sınıflı : paired t test
    # numeri amaç değişkeni - kategorik bagımsız değişken 2 den fazla sınıflı : anova
    def two_independent_samples_t_test(self,variable=None,x=None,y=None):
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        for i in df_numeric.columns:
            df_new= self.data.dropna(subset=[i])
            A=df_new[df_new[variable]==x][i]
            B=df_new[df_new[variable]==y][i]
            t, p = stats.ttest_ind(A, B, equal_var=False)
            print("ttest_ind: i=%s t = %g  p = %g" % (i,t, p))
            alpha = 0.05
            if p>alpha:
                print(i,"ile", variable," değişkeni arasında istatistiksel olarak anlamlı bir fark vardır.(Fail to Reject H0)")
            else:
                print(i," ile ", variable," değişkeni arasında istatistiksel olarak anlamlı bir fark yoktur.(reject H0)")
        print("*****************************************************************************************")
    # kategorik amaç değişkeni- kategorik bağımsız değişken : ki- kare bağımsızlık testi(chi-square)
    def chi_square_t_test(self,x=None,y=None):
        data_cross_tab=pd.crosstab(index=data[x],columns=data[y])
        chi2,p,dof,expected=stats.chi2_contingency(data_cross_tab)
        results=[["Item","Value"],
                 ["Chi-Square Test",chi2],
                 ["p - value",p]]
        print("Chi-Square Test =%g p=%g" %(chi2,p))
        alpha = 0.05
        if p>alpha:
            print(x,"ve",y," degiskenleri birbirinden bağımsızdır.(Fail to Reject H0)")
        else:
            print(x,"ve",y," degiskenleri birbirinden bağımsız değildir(reject H0)")
        print("*****************************************************************************************")
    def spearmanr_test(self,x=None):#  Korelasyon Anlamlılığı Testi için Spearman Testini kullanıcaz.---nonparametrik bir test
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        for i in df_numeric.columns:
            test_istatistigi,pvalue=stats.spearmanr(self.data[x],self.data[i])
            print('Korelasyon Katsayısı= %.4f, p- degeri=%.4f' % (test_istatistigi,pvalue))
            alpha=0.05
            if pvalue >alpha:
                print('Degiskenler arasında anlamlı bir fark yoktur, (Fail to reject)')
            else:
                print('Degiskenler arasında anlamlı bir fark vardır, (Reject)')
        print("*****************************************************************************************")

    #numeric amaç değişkeni- numeric bağımsız değişken : korelasyon analizi ve ilgili anlamlılık testi
    def pearsonr_test(self,x=None):#Korelasyon analizini pearsonr ile de gerçekleştirdim--parametrik test
        df_numeric=self.data.select_dtypes(include=['float64','int64'])
        for i in df_numeric.columns:
            test_istatistigi,pvalue=pearsonr(self.data[x],self.data[i])
            print('Korelasyon Katsayısı= %.4f, p- degeri=%.4f' % (test_istatistigi,pvalue))
            alpha=0.05
            if pvalue >alpha:
                print('Degiskenler arasında anlamlı bir fark yoktur, (Fail to reject)')
            else:
                print('Degiiskenler arasında anlamlı bir fark vardır, (Reject)')
        print("*****************************************************************************************")
# Data Preprocessing
class PreprocessStrategy:
    def __init__(self,data):
        self.data=data
    def fill_missing_value(self,data=None):
        data=self.data.dropna()
        return data.head()
    def drop_feature(self,x=None,y=None,z=None,data=None):
        X_ = self.data.drop([x, y, z],axis=1)
        return X_
    def select_feature(self,x=None):
      y = self.data[x]
      return y
    def data_concat(self,x=None,y=None,z=None,a=None,b=None,c=None):
      X = pd.concat([X_, dms[[x,y,z,a,b,c]]], axis = 1)
      return X
    def dataset_split(self,X=None,y=None,test_size=None,random_state=None):
      X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=test_size,random_state=random_state)
      print("X_train shape :",X_train.shape)
      print("X_test shape :", X_test.shape)
      print("y_train shape: ",y_train.shape)
      print("y_test shape :" ,y_test.shape)
      return X_train,X_test,y_train,y_test
    def replace_nan(self,x=None):
        return self.data.replace(x,np.nan,inplace=True)
    def fill_missing_specific_variable_with_median(self,X=None):#filling missing observations specific to the variable
        return self.data[X].fillna(self.data[X].median(),inplace=True)
    def fill_missing_value_with_mean(self):#fill in missing values in all variables with mean
        return self.data.apply(lambda x: x.fillna(x.mean()),axis=0)
    def fill_missing_value_with_median(self):#fill in missing values in all variables with median
        return self.data.apply(lambda x: x.fillna(x.median()),axis=0)
    def normalization(self):# converts variable values from 0 to 1
        return preprocessing.normalize(self.data)
    def one_hot_dummy_variable(self,x=None,y=None):#It can be used to convert categorical variable to continuous variable. As a result, awareness among classes will be preserved.
        df_one_hot=self.data.copy()
        dms = pd.get_dummies(df_one_hot[[x, y]])
        return dms
    def label_encoder(self,new_variable_name=None,categorical_variable_to_converted=None):# Performs conversions by the number of classes available
        lbe=preprocessing.LabelEncoder()
        data[new_variable_name]=lbe.fit_transform(data[categorical_variable_to_converted])
        return data[new_variable_name]
    def standardization(self):#a standardization is performed with an average of 0 standard deviations of one
        df_standardization=preprocessing.scale(self.data)
        return df_standardization
    def min_max_transformation(self,x=None,y=None):#Used to convert the values of a variable between two ranges that we want
        scaler=preprocessing.MinMaxScaler(feature_range=(x,y))
        return scaler.fit_transform(self.data)
    def discarding_pvalue(self,X_train=None,X_test=None):
      X_train = X_train[loj_model.pvalues[loj_model.pvalues <  0.05].index]
      X_test=X_test[loj_model.pvalues[loj_model.pvalues <  0.05].index]
      return X_train,X_test
    def binarize_transformation(self,threshold=None):#Converts the variable's values to 0 or 1 according to a certain threshold value
        binarizer=preprocessing.Binarizer(threshold=threshold).fit(self.data)
        return binarizer.transform(self.data)
  
        
# Data Modelling ,Performance/Evaluation metrics of the models
class GridSearchHelper():# model fit, model predict and model results are performed in this section
    def __init__(self,data):
        self.data=data
    def linear_regresyon(self,X_train=None,X_test=None,y_test=None,y_train=None):
        #X= df[[x]]# x independent variable
        #y=df[[y]] # y dependent variable 
        lm=LinearRegression()
        linear_model=lm.fit(X_train,y_train)# model object created
        return linear_model
    def pca(self,X_train=None,X_test=None,y_test=None,y_train=None):
        pca =PCA()
        X_reduced_test= pca.fit_transform(scale(X_test))
        print(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)[0:5])
        lm=LinearRegression()
        y_train=y_train.fillna(y_train.mean())
        pca_model=lm.fit(X_reduced_test,y_train)
        return pca_model
    def logistic_regresyon(self,X_train=None,y_train=None,solver=None):
        loj=LogisticRegression(solver=solver)
        loj_model=loj.fit(X_train,y_train)
        return loj_model
    def loj_summary(self,X_train=None,y_train=None):
      loj = sm.Logit(y_train, X_train)
      loj_model = loj.fit()
      print(loj_model.summary())
    def decision_tree(self,X_train=None,y_train=None):
      cart=DecisionTreeClassifier()
      cart_model=cart.fit(X_train,y_train)
      print(cart_model)
      return cart_model
    def random_forest(self,X_train=None,y_train=None):
      rf=RandomForestClassifier()
      rf_model=rf.fit(X_train,y_train)
      print(rf_model)
      return rf_model
    def decision_tree_model_tuning(self,X_train=None,y_train=None):
      decision_tree_classifier_grid={"max_depth": range(1,10),"criterion":["gini","entropy"],"min_samples_split":list(range(2,15))}
      cart=tree.DecisionTreeClassifier()
      cart_cv=GridSearchCV(cart,decision_tree_classifier_grid,cv=10,n_jobs=-1,verbose=2)
      cart_cv_model=cart_cv.fit(X_train,y_train)
      cart_classifier=tree.DecisionTreeClassifier(max_depth=cart_cv_model.best_params_["max_depth"],min_samples_split=cart_cv_model.best_params_["min_samples_split"],criterion=cart_cv_model.best_params_["criterion"])
      cart_tuned=cart_classifier.fit(X_train,y_train)
      print(cart_tuned)
      return cart_tuned
    def random_forest_model_tuning(self,X_train=None,y_train=None):
      rf_params={"max_depth":[3,5,8],"max_features":[0.1,0.25,0.5],"n_estimators":[200,500,1000],"min_samples_split":[5,10]}# göz öünde bulunacak olan degsken sayısı
      rf_cv_model=GridSearchCV(rf_model,rf_params,cv=10,n_jobs=-1,verbose=2)
      rf_cv_model.fit(X_train,y_train)
      rf_tuned=RandomForestClassifier(max_depth=rf_cv_model.best_params_["max_depth"],max_features=rf_cv_model.best_params_["max_features"],min_samples_split= rf_cv_model.best_params_["min_samples_split"],n_estimators= rf_cv_model.best_params_["n_estimators"])
      rf_tuned=rf_tuned.fit(X_train,y_train)
      print(rf_tuned)
      return rf_tuned
    def neural_network(self,X_train=None,y_train=None):
      mlpc=MLPClassifier()
      mlpc_model=mlpc.fit(X_train_scaled,y_train)
      print(mlpc_model)
      return mlpc_model
    def neural_network_model_tuning(self,X_train=None,y_train=None):
      mlpc_params={"alpha":[0.1,0.01,0.02],
                   "hidden_layer_sizes":[(100,100,100),(100,100),(3,5),(5,3)],"solver":["adam","sgd"],"activation":["relu","logistic"]}#activation function
      mlpc=MLPClassifier()
      mlpc_cv_model=GridSearchCV(mlpc,mlpc_params,cv=10,n_jobs=-1,verbose=2)
      mlpc_cv_model.fit(X_train_scaled,y_train)
      mlpc_tuned=MLPClassifier(alpha=mlpc_cv_model.best_params_["alpha"],
                        hidden_layer_sizes=mlpc_cv_model.best_params_["hidden_layer_sizes"],
                        solver=mlpc_cv_model.best_params_["solver"],
                        activation=mlpc_cv_model.best_params_["activation"])
      mlpc_tuned=mlpc_tuned.fit(X_train_scaled,y_train)
      print(mlpc_tuned)
      return mlpc_tuned

    def support_vektor_machine(self,X_train=None,y_train=None):
      svm=SVC()
      svm_model=svm.fit(X_train,y_train)
      print(svm_model)
      return svm_model
    def svc_model_tuning(self,X_train=None,y_train=None):
      svc_params={"C":np.arange(2,5),"kernel":["linear","rbf"]}
      svc=SVC()
      svc_cv_model=GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose=2)
      svc_cv_model.fit(X_train,y_train)
      svc_tuned=SVC(kernel=svc_cv_model.best_params_["kernel"],C=svc_cv_model.best_params_["C"]).fit(X_train,y_train)
      print(svc_tuned)
      return svc_tuned
    def gaussian_naive_bayes(self,X_train=None,y_train=None):
      naive_bayes=GaussianNB()
      nb_model=naive_bayes.fit(X_train,y_train)
      print(nb_model)
      return nb_model
    def multinominal_naive_bayes(self,X_train=None,y_train=None):
      mnb = MultinomialNB()
      mnb_model = mnb.fit(X_train, y_train)
      print(mnb_model)
      return mnb_model
    def XGBoost(self,X_train=None,y_train=None):
      xgb_model=XGBClassifier()
      xgb_model.fit(X_train,y_train)
      print(xgb_model)
      return xgb_model
    def xgb_model_tuning(self,X_train,y_train):
      xgb_params={"n_estimators":[100,500,1000,2000],
           "subsample":[0.6,0.8,1.0],
           "max_depth":[3,4,5,6],
           "learning_rate":[0.1,0.01,0.02,0.05],
           "min_samples_split":[2,5,10]}
           xgb=XGBClassifier()
           xgb_cv_model=GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,verbose=2)
           xgb_cv_model.fit(X_train,y_train)
           xgb=XGBClassifier(n_estimators=xgb_cv_model.best_params_["n_estimators"],subsample=xgb_cv_model.best_params_["subsample"],max_depth=xgb_cv_model.best_params_["max_depth"],learning_rate=xgb_cv_model.best_params_["learning_rate"],min_samples_split=xgb_cv_model.best_params_["min_samples_split"])
           xgb_tuned=xgb.fit(X_train,y_train)
           print(xgb_tuned)
           return xgb_tuned
    def knn(self,X_train=None,y_train=None):
      knn=KNeighborsClassifier()
      knn_model=knn.fit(X_train,y_train)
      print(knn_model)
      return knn_model
    def knn_model_tuning(self,X_train=None,y_train=None):
      knn_params={"n_neighbors":np.arange(2,40),"metric":["minkowski","euclidean","manhattan"]}
      knn=KNeighborsClassifier()
      knn_cv= GridSearchCV(knn,knn_params,cv=10)
      knn_cv.fit(X_train,y_train)
      knn_tuned=KNeighborsClassifier(n_neighbors=knn_cv.best_params_["n_neighbors"],metric=knn_cv.best_params_["metric"])
      knn_tuned.fit(X_train,y_train)
      print(knn_tuned)
      return knn_tuned
    def coefficiant(self,model_name=None):
      print("Sabit katsayı: ",loj_model.intercept_)# sabit katsayı
      print("katsayılar :",loj_model.coef_)# katsayılar
    def fit_predict_and_show_result(self,model_name=None,X_test=None):
      y_pred=model_name.predict(X_test)
      print("Classification Report",classification_report(y_test, y_pred))
      print("Accuracy Score ",accuracy_score(y_test, y_pred))
      print("roc_auc",roc_auc_score(y_test,model_name.predict(X_test)))

data=pd.read_csv("hmelq.csv")
loading_result=Loading_Data(data)
loading_result.translate_to_dataframe()

data_information=Information(data)
data_information.info_data()
data_information.describe_missing_values()
data_information.select_dtypes_numeric()
data_information.describe_data()
data_information.select_dtypes_category()

data_visualization=Visualizer(data)
data_visualization.msno_bar()
data_visualization.bar_plot(x="job",y="loan",z="bad")
data_visualization.bar_plot(x="job",y="loan",z="reason")
data_visualization.bar_plot(x="job",y="loan",z="bad")
data_visualization.bar_plot(x="job",y="loan",z="reason")

data_visualization=Visualizer(data)
data_visualization.msno_bar()

data_visualization.box_plot()
data_visualization.hist_plot()
data_visualization.dist_plot()
data_visualization.reg_plot()

data_visualization.count_plot(x="reason",z="bad")
data_visualization.count_plot(x="job",z="bad")

data_visualization.correlation()

data_visualization.scatter_plot(x="mortdue",y="value")

data_visualization.lm_plot(x="clage", y="clno", z="bad")
data_visualization.lm_plot(x="mortdue", y="loan", z="bad")
data_visualization.lm_plot(x="mortdue", y="value", z="bad",w= "reason")

data_visualization.pair_plot(z="bad")

hypothesis_testing=HypothesisTesting(data)
hypothesis_testing.normality_assumption()

hypothesis_testing.normality_assumption()

hypothesis_testing.assumption_of_variance_homogeneity(variable="bad",x=1,y=0)

hypothesis_testing.two_independent_samples_t_test(variable="bad",x=1,y=0)

hypothesis_testing.chi_square_t_test(x="bad",y="job")

# Sınıflandırma Algoritmaları

"""## Logistic Regression"""

# veri setinden na değerleri atıldı
df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)

info= Information(data)
info.print_shape()

grid_loj=GridSearchHelper(data)
grid_loj.logistic_regresyon(X_train,y_train,solver="liblinear")

# stats model ile modelleme
loj = sm.Logit(y_train, X_train)
loj_model = loj.fit()
loj_model.summary()
# p değerinin 0.05 den küçük olan değerler bizim için y yi aciklama da anlam ifade etmektedir.

preproses= PreprocessStrategy(data)
preproses.discarding_pvalue(X_train,X_test)

# Modelin kurulması
grid_loj=GridSearchHelper(data)
loj_model=grid_loj.logistic_regresyon(X_train,y_train,solver="liblinear")

katsayı= GridSearchHelper(data)
katsayı.coefficiant(model_name=loj_model)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=loj_model,X_test=X_test)

y_pred = loj_model.predict(X_test)
confusion_matrix = confusion_matrix(y_test, y_pred)
f,ax = plt.subplots(figsize=(5, 4))
sns.heatmap(confusion_matrix, annot=True, linewidths=0.01,cmap="YlGnBu",linecolor="gray", fmt= '.1f',ax=ax)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

loj_model.predict_proba(X_test)[0:10]
# 0 olma ve 1 olma ihtimallerini(olasılıgını) veriyor
#sağ 1 olma ihtimali 
#sol  ise 0 olma ihtimalidir
# threshold=0.5 için ayarlanmıştır

y[0:10]# gerçek y değerleri

# gerçek y değerlerim ile yukarıdaki tahmin değerleri  ile kıyaslamak için;
y_probs = loj_model.predict_proba(X_test)
y_probs = y_probs[:,1]
y_probs[0:10]
# değerlerin 1 olma olasılıkları

# threshold değerini değiştirerek tahmini gerçekleştirebiliriz
y_pred = [1 if i > 0.6 else 0 for i in y_probs]
y_pred[0:10]

#roc curve ün 1e yakın olmasını gerekiyor.
from sklearn.metrics import roc_auc_score,roc_curve
logit_roc_auc=roc_auc_score(y_test,loj_model.predict(X_test))
logit_roc_auc

visu=Visualizer(data)
visu.plot_roc_curve(model_adı=loj_model,X_test=X_test)

"""# Decision Tree Classifier"""

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)
info= Information(data)
info.print_shape()

# Hiç bir değişkeni atmadan decision tree classifier algoritmasını denedim.

# Modelin kurulması
grid_decision_tree=GridSearchHelper(data)
cart_model=grid_decision_tree.decision_tree(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=cart_model,X_test=X_test)

# model tuning
model_tuning=GridSearchHelper(data)
cart_tuned=model_tuning.decision_tree_model_tuning(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=cart_tuned,X_test=X_test)

visu=Visualizer(data)
visu.plot_tree(model_name=cart_tuned)

"""# Random Forest Classification"""

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)

info= Information(data)
info.print_shape()

grid_random_forest_tree=GridSearchHelper(data)
rf_model=grid_random_forest_tree.random_forest(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=rf_model,X_test=X_test)

# Model Tuning

model_tuning=GridSearchHelper(data)
rf_tuned=model_tuning.random_forest_model_tuning(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=rf_tuned,X_test=X_test)

visu=Visualizer(data)
visu.degisken_onem_görsellestirme(model_name=rf_tuned,X_train=X_train)

"""# Neural Network (Classification)"""

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)

info= Information(data)
info.print_shape()

scaler=StandardScaler()

scaler.fit(X_train)
X_train_scaled=scaler.transform(X_train)
X_test_scaled=scaler.transform(X_test)

X_train_scaled[0:2]

grid_neural_network=GridSearchHelper(data)
mlpc=grid_neural_network.neural_network(X_train,y_train)

mlpc.coefs_

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=mlpc,X_test=X_test)

# model tuning 
model_tuning=GridSearchHelper(data)
neural_network_tuned=model_tuning.neural_network_model_tuning(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=neural_network_tuned,X_test=X_test)

"""# Support Vector Classifier"""

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)

info= Information(data)
info.print_shape()

grid_svm=GridSearchHelper(data)
svm_model=grid_svm.support_vektor_machine(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=svm_model,X_test=X_test)

# model tuning svm
model_tuning=GridSearchHelper(data)
svm_tuned=model_tuning.svc_model_tuning(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=svm_model,X_test=X_test)

# Gaussian NaiveBayes (Classification)

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)
info= Information(data)
info.print_shape()

gaussian_naive_bayes
grid_svm=GridSearchHelper(data)
gausian_naive_bayes_model=grid_svm.gaussian_naive_bayes(X_train,y_train)

gausian_naive_bayes_model.predict_proba(X_test)[0:10]# sınıfların olasılık degerleri

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=gausian_naive_bayes_model,X_test=X_test)

# Multinominal Naive Bayes

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)
info= Information(data)
info.print_shape()

grid_multinominal=GridSearchHelper(data)
multinominal_model=grid_multinominal.multinominal_naive_bayes(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=multinominal_model,X_test=X_test)

# XGBoost (Classification)

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)
info= Information(data)
info.print_shape()

xgboost=GridSearchHelper(data)
xgb_model=xgboost.XGBoost(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=xgb_model,X_test=X_test)

# model tuning

model_tuning=GridSearchHelper(data)
xgb_tuned=model_tuning.xgb_model_tuning(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=xgb_tuned,X_test=X_test)

# KNN

df=data.copy()
df=df.dropna()
df.head()

dms = pd.get_dummies(df[['reason', 'job']])
dms.head()

X_ = df.drop(['bad', 'reason', 'job'],axis=1)
y = df["bad"]
X = pd.concat([X_, dms[["reason_DebtCon", "job_Mgr","job_Office","job_ProfEx","job_Sales","job_Self"]]], axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.28, random_state=42)
info= Information(data)
info.print_shape()

knn=GridSearchHelper(data)
knn_model=knn.knn(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=knn_model,X_test=X_test)

model_tuning=GridSearchHelper(data)
knn_tuned=model_tuning.knn_model_tuning(X_train,y_train)

estimate=GridSearchHelper(data)
estimate.fit_predict_and_show_result(model_name=knn_tuned,X_test=X_test)

# Tüm Sonuçların Karşılaştırılması

modeller=[linear_model,
          pca_model,
          loj_model,
          cart_model,
          rf_model,
          cart_tuned,
          rf_tuned,
          mlpc_model,
          mlpc_tuned,
          svc_model,
          svc_tuned,
          nb_model,
          mnb_model,
          xgb_model,
          xgb_tuned,
          knn_model,
          knn_tuned]

for model in modeller:
    isimler=model.__class__.__name__
    y_pred=model.predict(X_test)
    dogruluk=accuracy_score(y_test,y_pred)
    print("_" *28)
    print(isimler +":")
    print("Accuracy:{:.4%}".format(dogruluk))